from config import API_ID, API_HASH, FREEMIUM_LIMIT, PREMIUM_LIMIT, OWNER_ID, DEFAULT_SESSION, KS_GRAPHQL_QUERY
import yt_dlp
import requests
import os
import tempfile
import string
from devgagan import app, userrbot
from pyrogram import Client,filters
from pyrogram import enums
from pyrogram.types import MessageEntity
from devgagan.modules.shrink import is_user_verified
import random
import asyncio
from devgagan.core.func import *
import json
from devgagan.crawlers.douyin.web.web_crawler import DouyinWebCrawler
from devgagan.crawlers.hybrid.hybrid_crawler import HybridCrawler
from devgagan.crawlers.bilibili.web.web_crawler import BilibiliWebCrawler
from devgagan.crawlers.parser import parse_video_share_url, parse_video_id, VideoSource
import logging
import re

logger = logging.getLogger(__name__)
ongoing_downloads = {}

async def process_video(message, url):
    start_time = time.time()
    logger.info(f"process link: {url}")

    cookies = os.environ.get(str(message.chat.id) + '_cookie', '')

    api_data = await HybridCrawler().hybrid_parsing_single_video(url, True, cookies)
    download_url = api_data['video_data']['nwm_video_url_HQ']
    logger.info(api_data)
    await reply_dowload_url(message, download_url)

async def precheck(app, message):
    # å¿…é¡»å…³æ³¨é¢‘é“
    join = await subscribe(app, message)
    if join == 1:
        return False
    # ä¼šå‘˜æˆ–è€…æœ‰å…è´¹ä½“éªŒèµ„æ ¼
    user_id = message.chat.id
    freecheck = await chk_user(message, user_id)
    if freecheck == 1 and FREEMIUM_LIMIT == 0 and user_id not in OWNER_ID and not await is_user_verified(user_id):
        await message.reply("è¯·è´­ä¹°ä¼šå‘˜æˆ–è€…é€šè¿‡/tokenæŒ‡ä»¤è·å¾—å…è´¹ä½“éªŒèµ„æ ¼")
        return False
    # åˆ¤æ–­æ˜¯å¦æœ‰ä»»åŠ¡è¿›è¡Œä¸­
    if user_id in ongoing_downloads:
        await message.reply("**å·²æœ‰è¿›è¡Œä¸­çš„ä¸‹è½½ä»»åŠ¡,è¯·ç­‰å¾…ä»»åŠ¡ç»“æŸåå†å¼€å§‹æ–°çš„ä»»åŠ¡!**")
        return False

async def process_bilibili_video(message, url):
    logger.info(f"process link: {url}")

    cookies = os.environ.get(str(message.chat.id) + '_cookie', '')

    bv_id = await BilibiliWebCrawler().extract_bvid(url)
    if len(bv_id) == 0:
        await message.reply("ä¸‹è½½å¤±è´¥ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„è§†é¢‘é“¾æ¥")
        return
    api_data = await BilibiliWebCrawler().fetch_one_video(bv_id)
    cid = api_data['data']['cid']
    if cid == 0:
        await message.reply("ä¸‹è½½å¤±è´¥ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„è§†é¢‘é“¾æ¥")
        return
    result = await BilibiliWebCrawler().fetch_video_playurl(bv_id, str(cid))
    download_url = result['data']['dash']['video'][0]['baseUrl']
    await reply_dowload_url(message, download_url)

async def real_process_kuaishou_or_xhs(message, url):
        video_info = await parse_video_share_url(url)
        await reply_dowload_url(message, video_info.video_url)

async def fetch_video_info(url, ydl_opts):
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(url, download=False)
        return info_dict

def get_random_string(length=7):
    characters = string.ascii_letters + string.digits
    return ''.join(random.choice(characters) for _ in range(length)) 

async def reply_dowload_url(message, download_url):
    await message.reply(download_url, entities=[
        MessageEntity(
            type=enums.MessageEntityType.URL,
            offset=0,
            length=len(download_url)
            )
        ])

async def process_youtube(message, url):

    cookies = os.environ.get(str(message.chat.id) + '_ytcookie', None)

    random_filename = get_random_string() + ".mp4"
    download_path = os.path.abspath(random_filename)
    logger.info(f"Generated random download path: {download_path}")

    temp_cookie_path = None
    if cookies:
        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.txt') as temp_cookie_file:
            temp_cookie_file.write(cookies)
            temp_cookie_path = temp_cookie_file.name
        logger.info(f"Created temporary cookie file at: {temp_cookie_path}")

    thumbnail_file = None
    metadata = {'width': None, 'height': None, 'duration': None, 'thumbnail': None}

    ydl_opts = {
            'outtmpl': download_path,
            'format': 'best',
            'cookiefile': temp_cookie_path if temp_cookie_path else None,
            'writethumbnail': True,
            'verbose': True,
            }

    info_dict = await fetch_video_info(url, ydl_opts)
    if not info_dict:
        await message.reply("ä¸‹è½½å¤±è´¥,è¯·ç¨åé‡è¯•")
        return
    if 'url' in info_dict:
        download_url = info_dict['url']
        await reply_dowload_url(message, download_url)
    else:
        await message.reply("ä¸‹è½½å¤±è´¥,è¯·ç¨å€™é‡è¯•")

@app.on_message(filters.command("single") & filters.private)
async def process_single_video(_, message):
    result = await precheck(_, message)
    if result == False:
        return
    if len(message.text.split()) < 2:
        await message.reply("**ç”¨æ³•:** `/single <è§†é¢‘é“¾æ¥>`\n\nè¯·æŒ‰è¦æ±‚è¾“å…¥æŒ‡ä»¤!")
        return
    url = message.text.split()[1]
    logger.info(f"Received link: {url}")
    ongoing_downloads[message.chat.id] = True
    try:
        if "douyin" in url or "tiktok" in url:
            await process_video(message, url)
        elif "bilibili" in url or "b23.tv" in url:
            await process_bilibili_video(message, url)
        elif "youtube" in url:
            await process_youtube(message, url)
        else:
            await real_process_kuaishou_or_xhs(message, url)
    except Exception as e:
        await message.reply(f"**å‡ºé”™äº†:**{e}")
    finally:
        ongoing_downloads.pop(message.chat.id, None)

async def process_douyin_homepage(message, input_url, download_count):
    douyin_id = await DouyinWebCrawler().get_sec_user_id(input_url)
    if douyin_id == None or len(douyin_id) == 0:
        logger.error(f"input url is invalid: {input_url}")
        await message.reply("éæ³•urlï¼Œè¯·é‡æ–°è¾“å…¥")
        return
    current_count = 0
    page_size = 10
    cursor = 0
    while current_count < download_count:
        query_size = download_count - current_count
        if query_size > page_size:
            query_size = page_size
        api_data = await DouyinWebCrawler().fetch_user_post_videos(douyin_id, cursor, query_size)
        cursor = api_data['max_cursor']
        has_more = api_data['has_more']
        logger.info(f"cursor: {cursor}, hasmore:{has_more}")
        for video_data in api_data['aweme_list']:
            if current_count >= download_count:
                break
            if 'video' in video_data:
                await reply_dowload_url(message, video_data['video']['play_addr']['url_list'][2])
                current_count += 1
                time.sleep(0.1)
        if has_more != 1:
            break
        time.sleep(1)

def get_kuaishou_userid(url: str) -> str:
    """
    ä»å¿«æ‰‹ç”¨æˆ·ä¸»é¡µURLä¸­æå–userid
    :param url: å¿«æ‰‹ç”¨æˆ·ä¸»é¡µURLï¼ˆæ”¯æŒPCç«¯å’Œç§»åŠ¨ç«¯æ ¼å¼ï¼‰
    :return: ç”¨æˆ·useridå­—ç¬¦ä¸²
    """
    # åŒ¹é…ä¸¤ç§å¸¸è§URLæ ¼å¼ï¼ˆå«PCç«¯å’Œç§»åŠ¨ç«¯ï¼‰
    patterns = [
        r'profile/([a-zA-Z0-9]+)',  # PCç«¯ç¤ºä¾‹ï¼šhttps://live.kuaishou.com/profile/xxx[9](@ref)
        r'short-video/([a-zA-Z0-9]+)'  # ç§»åŠ¨ç«¯ç¤ºä¾‹ï¼šhttps://v.kuaishou.com/xxx
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    logger.error(f"invalid kuaishou homepage url: {url}")
    return ""

async def process_kuaishou_homepage(message, input_url, download_count):
    user_id = get_kuaishou_userid(input_url)
    if len(user_id) == 0:
        message.reply(message.chat.id, "è¯·è¾“å…¥æ­£ç¡®çš„ä¸»é¡µurl")
        return

    url = "https://www.kuaishou.com/graphql"
    cookies = os.environ.get(str(message.chat.id) + '_cookie', None)
    headers = {
            'Accept': '*/*',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Content-Type': 'application/json',
            'Cookie': cookies,
            'Host': 'www.kuaishou.com',
            'Origin': 'https://www.kuaishou.com',
            'Referer': f'https://www.kuaishou.com/profile/{user_id}',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36',
            'sec-ch-ua': '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
            }

    current_count = 0
    pcursor = None
    while current_count < download_count:
        payload = {
                "operationName": "visionProfilePhotoList",
                "variables": {
                    "userId": user_id,
                    "pcursor": pcursor if pcursor else None,
                    "page": "profile"
                    },
                "query": KS_GRAPHQL_QUERY
    }
        response = requests.post(
                url,
                headers=headers,
                json=payload  # è‡ªåŠ¨å¤„ç†Content-Typeå’ŒJSONåºåˆ—åŒ–
                )
        response.raise_for_status()
        data = response.json()
        if data.get('data'):
            for feed in data['data']['visionProfilePhotoList']['feeds']:
                if current_count >= download_count:
                    break
                await reply_dowload_url(message, feed['photo']['photoUrl'])
                time.sleep(0.1)
                current_count += 1
            pcursor = data['data']['visionProfilePhotoList']['pcursor']
        time.sleep(1)

async def process_youtube_post_list(message, user_url, max_count):
    """
    è·å–ç”¨æˆ·ä¸»é¡µè§†é¢‘ä¸‹è½½åœ°å€

    å‚æ•°ï¼š
    user_url - ç”¨æˆ·ä¸»é¡µ/é¢‘é“é“¾æ¥
    max_count - éœ€è¦è·å–çš„è§†é¢‘æ•°é‡ï¼ˆé»˜è®¤5ï¼‰
    cookies_file - Cookieæ–‡ä»¶è·¯å¾„
    browser_name - æµè§ˆå™¨åç§°
    """
    cookies = os.environ.get(str(message.chat.id) + '_ytcookie', None)
    temp_cookie_path = None
    if cookies:
        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.txt') as temp_cookie_file:
            temp_cookie_file.write(cookies)
            temp_cookie_path = temp_cookie_file.name
        logger.info(f"Created temporary cookie file at: {temp_cookie_path}")

    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šè·å–è§†é¢‘åˆ—è¡¨
        list_opts = {
                'noplaylist': False,
                'extract_flat': 'in_playlist',
                'playlist_items': f'1-{max_count}',
                'quiet': True,
                'no_warnings': True,
                'cookiefile': temp_cookie_path if temp_cookie_path else None,
                }

        with yt_dlp.YoutubeDL(list_opts) as ydl:
            # è·å–ç”¨æˆ·ä¸»é¡µè§†é¢‘åˆ—è¡¨
            list_info = ydl.extract_info(user_url, download=False)

            if not list_info or 'entries' not in list_info:
                logger.error("âŒ æœªæ‰¾åˆ°è§†é¢‘åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æ­£ç¡®")
                message.reply("âŒ æœªæ‰¾åˆ°è§†é¢‘åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æ­£ç¡®")
                return

            videos = list_info['entries'][:max_count]
            logger.info(f"ğŸ¬ å…±æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘\n")

            # ç¬¬äºŒé˜¶æ®µï¼šé€ä¸ªè·å–è§†é¢‘è¯¦æƒ…
            detail_opts = {
                    'noplaylist': True,
                    'quiet': True,
                    'no_warnings': True,
                    'cookiefile': temp_cookie_path if temp_cookie_path else None,
                    }

            with yt_dlp.YoutubeDL(detail_opts) as detail_ydl:
                for idx, video in enumerate(videos, 1):
                    if not video.get('url'):
                        continue

                    try:
                        # è·å–å•ä¸ªè§†é¢‘è¯¦æƒ…
                        video_info = detail_ydl.extract_info(video['url'], download=False)

                        if 'formats' in video_info:
                            best_url = next(
                                    (f['url'] for f in reversed(video_info['formats'])
                                        if f.get('url')),
                                    None
                                    )
                            if best_url:
                                await reply_dowload_url(message, best_url)
                            else:
                                logger.error("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆä¸‹è½½åœ°å€")
                        else:
                            logger.error("âš ï¸ è¯¥è§†é¢‘å¯èƒ½å—è®¿é—®é™åˆ¶")

                    except yt_dlp.utils.DownloadError as e:
                        logger.error(f"âŒ è·å–å¤±è´¥ï¼š{str(e)}")
                        continue

    except Exception as e:
        logger.error(f"å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼š{str(e)}")
        message.reply(f"å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼š{str(e)}")


# ä¸‹è½½ç”¨æˆ·é¦–é¡µçš„è§†é¢‘
@app.on_message(filters.command("userpost") & filters.private)
async def process_homepage_download(_, message):
    # å¿…é¡»å…³æ³¨é¢‘é“
    join = await subscribe(_, message)
    if join == 1:
        return
    # ä¼šå‘˜æ‰èƒ½æ“ä½œ
    user_id = message.chat.id
    freecheck = await chk_user(message, user_id)
    if freecheck == 1 and user_id not in OWNER_ID:
        await message.reply("**ä»…ä¼šå‘˜å¯æ“ä½œ!**")
        return
    # åˆ¤æ–­æ˜¯å¦æœ‰ä»»åŠ¡è¿›è¡Œä¸­
    if user_id in ongoing_downloads:
        await message.reply("**å·²æœ‰è¿›è¡Œä¸­çš„ä¸‹è½½ä»»åŠ¡,è¯·ç­‰å¾…ä»»åŠ¡ç»“æŸåå†å¼€å§‹æ–°çš„ä»»åŠ¡!**")
        return

    if len(message.text.split()) < 3:
        await message.reply("**ç”¨æ³•:** `/userpost <ç”¨æˆ·ä¸»é¡µurl> <è¦ä¸‹è½½è§†é¢‘æ•°é‡>`\n\nè¯·æŒ‰ç…§è¦æ±‚è¾“å…¥æŒ‡ä»¤!")
        return

    input_url = message.text.split()[1]
    download_count = int(message.text.split()[2])
    ongoing_downloads[user_id] = True

    try:
        if 'douyin' in input_url:
            await process_douyin_homepage(message, input_url, download_count)
        elif 'kuaishou' in input_url:
            await process_kuaishou_homepage(message, input_url, download_count)
        elif 'youtube' in input_url:
            await process_youtube_post_list(message, input_url, download_count)
    except Exception as e:
        await message.reply(f"**å‡ºé”™äº†:**{e}")
    finally:
        ongoing_downloads.pop(user_id, None)

@app.on_message(filters.command("setcookie") & filters.private)
async def setcookie(_, message):
    cookie = ''
    while True:
        msg = await app.ask(message.chat.id, "è¯·è¾“å…¥æˆ–è€…è¡¥å……cookie, æ²¡æœ‰è¡¥å……è¯·è¾“å…¥**done**")
        if msg.text == 'done':
            break
        cookie += msg.text
    os.environ[str(message.chat.id) + '_cookie'] = cookie
    await app.send_message(message.chat.id, "è®¾ç½®æˆåŠŸ")

@app.on_message(filters.command("setytcookie") & filters.private)
async def setcookie(_, message):
    tmp = '' 
    while True:
        cookie = await app.ask(message.chat.id, "è¯·è¾“å…¥æˆ–è€…è¡¥å……cookie, æ²¡æœ‰è¡¥å……è¯·è¾“å…¥**__done__**")
        if cookie.text == 'done':
            break
        lines = cookie.text.split('\n')
        if len(lines) < 4:
            return
        tmp += lines[0] + '\n'
        tmp += lines[1] + '\n'
        tmp += lines[2] + '\n'
        for line in lines[3:]:
            new_line = ''
            for col in line.split(' '):
                if len(col) == 0:
                    continue
                if len(new_line) > 0:
                    new_line += '\t'
                new_line += col
            new_line += '\n'
            tmp += new_line
    os.environ[str(message.chat.id) + '_ytcookie'] = tmp
    logger.info(tmp)
    await app.send_message(message.chat.id, "è®¾ç½®æˆåŠŸ")
    return
